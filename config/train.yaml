################################################################
########### Dataset parameters ############################
################################################################
dataset:
  name: my_dataset # name of the dataset (must match folder name under $DATA_DIR)
  n_steps_input: 1 # number of input time steps
  n_steps_output: 16 # number of output time steps
  use_normalization: true # if true, normalize the data
  normalization_type: zscore # zscore or rms
  dt_stride: 1 # temporal stride (int or list of [min, max] for random stride)
  full_trajectory_mode: false # if true, use full trajectories instead of windows
  max_rollout_steps: 10000 # max steps for full trajectory mode
  skip_steady_states: false # if true, skip steady-state regions
  spatial_downsample_size: null # target spatial size (H, W) for downsampling, e.g. [32, 32]. null = no downsampling
  downsample_mode: bilinear # interpolation mode: bilinear, nearest, bicubic, area, nearest-exact

################################################################
########### Simulation parameters ############################
################################################################
time_limit: "24:00:00"

checkpoint:
  checkpoint_name: null # null, "latest", "best", or number of epoch to load

  # if true, load optimizer and lr scheduler states from checkpoint
  # if false, only load model weights, and reset optimizer and lr scheduler states. This is useful for fine-tuning.
  restart: true

################################################################
########### General parameters ############################
################################################################
mem_budget: 1 # memory budget as fraction of total memory for torch.compile activation checkpointing
seed: 42
batch_size: 64 # batch size per GPU!!!!
total_updates: 600e3 # number of batches to use for training
updates_per_epoch: 1000 # number of updates per epoch
checkpoint_every_updates: 1e3 # number of updates between checkpoints

amp: true # use automatic mixed precision
precision: bfloat16 # precision for automatic mixed precision (float16, bfloat16)
compile: true # if true, use torch.compile to compile the model (PyTorch 2.0+)
compile_mode: "default" # torch.compile mode: default, reduce-overhead, max-autotune, max-autotune-no-cudagraphs
compile_fullgraph: true # if true, compile the full graph (may break with dynamic control flow)

###############################################################
########### Model parameters ############################ ####
################################################################
model:
  size: S # DiT size preset: S (~33M), B (~130M), L (~458M), XL (~675M)

  in_channels: null # number of input channels, computed from dataset
  spatial_size: null # spatial size of the input (H, W), computed from dataset
  temporal_size: null # temporal size of the input, computed from dataset
  cond_dim: null # dimension of the conditioning vector

  # Flow matching scheduler
  scheduler:
    type: cond_ot
    params: {}


################################################################
########### Dataloader parameters ############################
################################################################
num_workers: 16 # number of workers for dataloader per GPU

################################################################
########### Optimizer parameters ############################
################################################################
optimizer:
  name: AdamW # optimizer name (AdamW, SGD)
  # Make sure to supply all parameters required by the optimizer
  learning_rate: 1e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]

max_grad_norm: 5.0 # null, or float for gradient clipping (stabilizes training)

################################################################
########### LRScheduler parameters ############################
################################################################
# Delete section to deactivate LR scheduling
lr_scheduler:
  first_stage:
    name: LinearLR # linear warmup scheduler
    start_factor: 0.001
    end_factor: 1.0
    num_updates: 5000 # number of updates for linear warmup
  second_stage:
    name: CosineAnnealingLR # cosine annealing scheduler
    num_updates: -1 # num batches for cosine annealing, -1 means use all remaining batches to wind down
    end_factor: 0.01 # percentage of initial learning rate to use as minimum learning rate
  third_stage:
    name: LinearLR # linear cool down scheduler
    end_factor: 0
    num_updates: 10 # number of batches for linear cool down

################################################################
########### WandB parameters ############################
################################################################
wandb: # run name is automatically set to directory name this file is in
  enabled: false
  project: my_ml_project
  entity: my_wandb_username
  tags:
    - train
    - test
  notes: "Training my first model"
